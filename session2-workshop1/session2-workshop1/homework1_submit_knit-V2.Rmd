---
title: "Session 2: Homework 1"
author: "Group 34: Muhammad Nauman Alam Khan, Tom Invernizzi, Rayna Zhang, Jerome Billiet, Christopher Baumann "
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_dept: 3
    toc_float: yes
    code_folding: show
---


```{r, setup, echo=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```


```{r load-libraries, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(fivethirtyeight)
library(here)
library(skimr)
library(janitor)
library(vroom)
library(tidyquant)
library(rvest)    # scrape websites
library(purrr)  
library(lubridate) #to handle dates
library(kableExtra)
```

# Where Do People Drink The Most Beer, Wine And Spirits?

Back in 2014, [fivethiryeight.com](https://fivethirtyeight.com/features/dear-mona-followup-where-do-people-drink-the-most-beer-wine-and-spirits/) published an article on alchohol consumption in different countries. The data `drinks` is available as part of the `fivethirtyeight` package. Make sure you have installed the `fivethirtyeight` package before proceeding.

```{r, load_alcohol_data}

# import data
library(fivethirtyeight)
data(drinks)

```

## Data Description    

**- What are the variable types? Any missing values we should worry about?**   

*ANSWER:*      
       
There are 4 numerics: beer_servings, spirit_servings, wine_servings and total_litres_of_pure_alcohol and 1 charcter: country. 

No missing values based on the data.

```{r glimpse_skim_data}

# initial data exploration
glimpse(drinks)
skim(drinks)

# check if missing values
anyNA(drinks)
```

## Top 25 Beer Consuming Countries      
        
**- Make a plot that shows the top 25 beer consuming countries**   

```{r beer_plot, fig.width=10,fig.height=5}

# rearrange and filter for the top 25
beer = drinks %>%
  arrange(desc(beer_servings)) %>% 
  top_n(25)

# plot the results
ggplot(beer, aes(x=reorder(country, beer_servings), y= beer_servings)) +
  geom_bar(stat="identity") +
  coord_flip() +
  theme_economist() +
  theme(legend.position = "none",
           axis.title.x = element_text()) + 
  labs(title = "Czech Republic is the top beer consuming country in the world\n",
       subtitle = "Top 25 beer consuming countries\n",
         x = "Country\n", 
         y = "\nBeer servings")

```

## Top 25 Wine Consuming Countries    
    
**- Make a plot that shows the top 25 wine consuming countries**     

```{r wine_plot, fig.width=10,fig.height=5}

# rearrange and filter for the top 25
wine = drinks %>%
  arrange(desc(wine_servings)) %>% 
  top_n(25)

# plot the results
ggplot(wine, aes(x=reorder(country, wine_servings), y= wine_servings)) +
  geom_bar(stat="identity") +
  coord_flip() +
  theme_economist() +
  theme(legend.position = "none",
           axis.title.x = element_text()) + 
  labs(title = "France is the top wine consuming country in the world\n",
       subtitle = "Top 25 wine consuming countries\n",
         x = "Country\n", 
         y = "\nWine servings")

```

## Top 25 Spirit Consuming Countries     

**- Finally, make a plot that shows the top 25 spirit consuming countries**    

```{r spirit_plot, fig.width=10,fig.height=5}

# rearrange and filter for the top 25
spirit = drinks %>%
  arrange(desc(spirit_servings)) %>% 
  top_n(25)

# plot the results
ggplot(wine, aes(x=reorder(country, spirit_servings), y= spirit_servings)) +
  geom_bar(stat="identity") +
  coord_flip() +
  theme_economist() +
  theme(legend.position = "none",
           axis.title.x = element_text()) + 
  labs(title = "Grenada is the top spirit consuming country in the world\n",
       subtitle = "Top 25 spirit consuming countries\n",
         x = "Country\n", 
         y = "\nSpirit servings")

```

## Interpretation     

**- What can you infer from these plots? Don't just explain what's in the graph, but speculate or tell a short story (1-2 paragraphs max).**    

*ANSWER:*

Based on the plots above, it becomes obvious that Czech Republic is the leading country in beer consumption, France for wine and Grenada for spirits. However, looking at all three plots together reveals interesting insights. Across all plots, European countries are clearly dominating and thus consuming relatively much in comparison to the rest of the world. Furthermore, cultural differences can be observed. For instance, France high wine does not imply that they also drink a lot of other alcoholic beverages (for beer they are only rank 25). Another interesting insight is the demand for strong liquids (spirits) in the eastern part of Europe as one can see in the third plot. The hypothesis that wine is a more expensive and exclusive beverage that is reserved for rich countries cannot be confirmed. 


# Analysis of movies- IMDB dataset

We will look at a subset sample of movies, taken from the [Kaggle IMDB 5000 movie dataset](https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset)

```{r,load_movies, warning=FALSE, message=FALSE}

# import data
movies <- read_csv(file = "data/movies.csv")

```

Besides the obvious variables of `title`, `genre`, `director`, `year`, and `duration`, the rest of the variables are as follows:

- `gross` : The gross earnings in the US box office, not adjusted for inflation
- `budget`: The movie's budget 
- `cast_facebook_likes`: the number of facebook likes cast members received
- `votes`: the number of people who voted for (or rated) the movie in IMDB 
- `reviews`: the number of reviews for that movie
- `rating`: IMDB average rating 

## Data Description  

**- Are there any missing values (NAs)? Are all entries distinct or are there duplicate entries?**    

*ANSWER:*   
   
No, there are no missing values (NAs). Depending on the variable, there are a few duplicates. The duplicates within titles could be due to movies that actually were given the same name. Hence, we are not going to remove those.

```{r}

# initial data exploration
glimpse(movies)
skim(movies)

# check for missing values
anyNA(movies)

```

## Movies by genre   

**- Produce a table with the count of movies by genre, ranked in descending order**   

```{r, fig.width=10,fig.height=5}

# table transformation
table1 = movies %>% 
  group_by(genre) %>% 
  summarise(count_genre=count(genre)) %>% 
  arrange(desc(count_genre))

# print table
table1 %>%
  kbl(col.names = c("Genre", "Count")) %>%
  kable_classic(c("hover"), full_width = F, html_font = "Cambria") %>%
  kable_styling()



```

## Average Gross Earning, Budget and `return_on_budget`by genre    

**- Produce a table with the average gross earning and budget (`gross` and `budget`) by genre. Calculate a variable `return_on_budget` which shows how many $ did a movie make at the box office for each $ of its budget. Ranked genres by this `return_on_budget` in descending order**    

```{r, fig.width=10,fig.height=5}

# table transformation
table2 = movies %>% 
  group_by(genre) %>% 
  summarise(avg_gross=mean(gross), avg_budget=mean(budget)) %>% 
  mutate(return_on_budget = avg_gross / avg_budget) %>% 
  mutate(avg_gross = avg_gross/1000000) %>%
  mutate(avg_budget = avg_budget/1000000) %>%
  arrange(desc(return_on_budget))

# print table
table2 %>%
  kable(col.names = c("Genre", "Avg. Gross (in USDm)", "Avg. Budet (in USDm)", "Return on budget")) %>%
  kable_classic(c("hover"),full_width = F, html_font = "Cambria") %>%
  kable_styling()
```


## Diectors with the Highes Gross Revenue

**- Produce a table that shows the top 15 directors who have created the highest gross revenue in the box office. Don't just show the total gross amount, but also the mean, median, and standard deviation per director.**

```{r, fig.width=10,fig.height=5}

# table transformation 
table3 = movies %>% 
  group_by(director) %>% 
  summarise(total_gross=sum(gross), avg_gross=mean(gross), std_gross=sd(gross), median_gross=median(gross)) %>% 
  arrange(desc(total_gross)) %>% 
  mutate(avg_gross = avg_gross/1000000) %>%
  mutate(total_gross = total_gross/1000000) %>%
  mutate(std_gross = std_gross/1000000) %>%
  mutate(median_gross = median_gross/1000000) %>%
  top_n(15)

# print table
table3 %>%
  kbl(col.names = c("Director", "Total Gross (in USDm)", "Avg. Gross (in USDm)", "SD Gross (in USDm)", "Median Gross (in USDm)")) %>%
  kable_classic(c("hover"), full_width = F, html_font = "Cambria") %>%
  kable_styling()

```


## Ratings by genre   

**- Finally, ratings. Produce a table that describes how ratings are distributed by genre. We don't want just the mean, but also, min, max, median, SD and some kind of a histogram or density graph that visually shows how ratings are distributed.**    

```{r, fig.width=10,fig.height=5}

# table transformation
table4 = movies %>% 
  group_by(genre) %>% 
  summarise(avg_rating=mean(rating), min_rating=min(rating), max_rating=max(rating), median_rating=median(rating), std_rating=sd(rating)) %>% 
  arrange(desc(avg_rating))

# print table
table4 %>%
  kbl(col.names = c("Genre", "Average", "Min", "Max", "Median", "SD")) %>%
  kable_classic(c("hover"), full_width = F, html_font = "Cambria") %>%
  kable_styling()


# histogram of the average ratings
ggplot(table4, aes(x=reorder(genre, avg_rating), y=avg_rating)) +
  geom_bar(stat="identity") +
  theme_economist() +
  coord_flip() +
  theme(legend.position = "none",
           axis.title.x = element_text()) + 
  labs(title = "Biography movies receive on average the highest ratings\n",
       subtitle = "Average rating by genre\n",
         x = "Genre\n", 
         y = "\nAverage rating")

```

## Relationship between `gross` and `cast_facebook_likes`

**- Examine the relationship between `gross` and `cast_facebook_likes`. Produce a scatterplot and write one sentence discussing whether the number of facebook likes that the cast has received is likely to be a good predictor of how much money a movie will make at the box office. What variable are you going to map to the Y- and X- axes?**

*ANSWER:*    
      
Although the trend line suggests a positive correlation, we do not think that this is a good predictor since the model  error seems to be very large which makes predictions less accurate.   

```{r, fig.width=10,fig.height=5}

# create ggplot and apply log10 to transform data
ggplot(data=movies, aes(x=cast_facebook_likes, y=gross)) +
  geom_point() +
  geom_smooth(method='lm', formula= y~x) +
  scale_x_log10() +
  scale_y_log10() +
  labs(title = "Relationship between gross revenue and cast facebook likes",
         x = "Cast facebook likes (log10)", 
         y = "Gross revenue (log10)")

```

## Relationship between `gross` and `budget`    

**- Examine the relationship between `gross` and `budget`. Produce a scatterplot and write one sentence discussing whether budget is likely to be a good predictor of how much money a movie will make at the box office.**
 
*ANSWER:*     
     
Upon analysing the scatterplot, it becomes evident that there is strong positive correlation between the budget and gross revenue, especially for high budget/ gross revenue.

```{r, gross_on_budget, fig.width=10,fig.height=5}

# create ggplot and apply log10 to transform data
ggplot(data=movies, aes(x=budget, y=gross)) +
  geom_point() +
  geom_smooth(method='lm', formula= y~x) +
  scale_x_log10() +
  scale_y_log10() +
  labs(title = "Relationship between gross revenue and budget",
         x = "Budget (log10)", 
         y = "Gross revenue (log10)")

```

## Relationship between `gross` and `rating`     

**- Examine the relationship between `gross` and `rating`. Produce a scatterplot, faceted by `genre` and discuss whether IMDB ratings are likely to be a good predictor of how much money a movie will make at the box office. Is there anything strange in this dataset?**

*ANSWER:*     
      
Whether the rating is a good predictor for the gross revenue depends on the category. For instance, for action movies the ratings seem to be a quite decent predictor while it is not the case for sci-fi movies due to very high variance. Additionally, certain genres such as documentary even show a negative correlation between ratings and gross revenue. Furthermore, there are several genres with a limited amount of data points which does not allow to make a inference about a possible prediction. 

```{r, gross_on_rating, fig.width=10,fig.height=5}

# create ggplot and facet by genre
ggplot(data=movies, aes(x=rating, y=gross)) +
  geom_point() +
  geom_smooth(method='lm', formula= y~x) +
  scale_y_log10() +
  labs(title = "Relationship between gross revenue and rating",
         x = "Rating", 
         y = "Gross revenue (log10)") +
  facet_wrap(~genre)

```

# Returns of financial stocks

We will use the `tidyquant` package to download historical data of stock prices, calculate returns, and examine the distribution of returns. 

We must first identify which stocks we want to download data for, and for this we must know their ticker symbol; Apple is known as AAPL, Microsoft as MSFT, McDonald's as MCD, etc. The file `nyse.csv` contains 508 stocks listed on the NYSE, their ticker `symbol`, `name`, the IPO  (Initial Public Offering) year, and the sector and industry the company is in.

```{r load_nyse_data, message=FALSE, warning=FALSE}

# import data
nyse <- read_csv(file="data/nyse.csv")

```

## Number of Companies per Sector     

**- Based on this dataset, create a table and a bar plot that shows the number of companies per sector, in descending order**

```{r companies_per_sector, fig.width=10,fig.height=5}

#import required libraries
library(tidyquant)

# table transformation
table_nyse <- nyse %>% 
  group_by(sector) %>% 
  summarise(sector_count=count(sector)) %>% 
  arrange(desc(sector_count))

# print table
table_nyse %>%
  kbl(col.names = c("Sector", "Count")) %>%
  kable_classic(c("hover"), full_width = F, html_font = "Cambria") %>%
  kable_styling()

# create ggplot bar chart
ggplot(table_nyse, aes(x=reorder(sector, sector_count), y=sector_count)) +
  geom_bar(stat="identity")+
  coord_flip() +
  theme_economist() +
  theme(legend.position = "none",
           axis.title.x = element_text()) + 
  labs(title = "The finance sector counts the most listed companies\n",
       subtitle = "Number of listed companies by sector\n",
         x = "Sector\n", 
         y = "\nNumber of companies")

```

Next, let's choose the [Dow Jones Industrial Aveareg (DJIA)](https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average) stocks and their ticker symbols and download some data. Besides the thirty stocks that make up the DJIA, we will also add `SPY` which is an SP500 ETF (Exchange Traded Fund).

```{r, tickers_from_wikipedia}

# define source url
djia_url <- "https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average"

#get tables that exist on URL
tables <- djia_url %>% 
  read_html() %>% 
  html_nodes(css="table")

# parse HTML tables into a dataframe called djia. 
# Use purr::map() to create a list of all tables in URL
djia <- map(tables, . %>% 
               html_table(fill=TRUE)%>% 
               clean_names())

# constituents
table1 <- djia[[2]] %>% # the second table on the page contains the ticker symbols
  mutate(date_added = ymd(date_added),
         
         # if a stock is listed on NYSE, its symbol is, e.g., NYSE: MMM
         # We will get prices from yahoo finance which requires just the ticker
         
         # if symbol contains "NYSE*", the * being a wildcard
         # then we jsut drop the first 6 characters in that string
         ticker = ifelse(str_detect(symbol, "NYSE*"),
                          str_sub(symbol,7,11),
                          symbol)
         )

# we need a vector of strings with just the 30 tickers + SPY
tickers <- table1 %>% 
  select(ticker) %>% 
  pull() %>% # pull() gets them as a sting of characters
  c("SPY")# %>% # and lets us add SPY, the SP500 ETF

```


```{r get_price_data, message=FALSE, warning=FALSE, cache=TRUE}
# Notice the cache=TRUE argument in the chunk options. Because getting data is time consuming, # cache=TRUE means that once it downloads data, the chunk will not run again next time you knit your Rmd

myStocks <- tickers %>% 
  tq_get(get  = "stock.prices",
         from = "2000-01-01",
         to   = "2020-08-31") %>%
  group_by(symbol) 

glimpse(myStocks) # examine the structure of the resulting data frame

```

Financial performance analysis depend on returns; If I buy a stock today for 100 and I sell it tomorrow for 101.75, my one-day return, assuming no transaction costs, is 1.75%. So given the adjusted closing prices, our first step is to calculate daily and monthly returns.

```{r calculate_returns, message=FALSE, warning=FALSE, cache=TRUE}
#calculate daily returns
myStocks_returns_daily <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "daily", 
               type       = "log",
               col_rename = "daily_returns",
               cols = c(nested.col))  

#calculate monthly  returns
myStocks_returns_monthly <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "monthly", 
               type       = "arithmetic",
               col_rename = "monthly_returns",
               cols = c(nested.col)) 

#calculate yearly returns
myStocks_returns_annual <- myStocks %>%
  group_by(symbol) %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "yearly", 
               type       = "arithmetic",
               col_rename = "yearly_returns",
               cols = c(nested.col))
```

## Summary of Montly Returns   

**- Create a dataframe and assign it to a new object, where you summarise monthly returns since 2017-01-01 for each of the stocks and `SPY`; min, max, median, mean, SD.**

```{r summarise_monthly_returns}

# table transformation and extraction of monthly returns
summarised_returns <- myStocks_returns_monthly %>% 
  group_by(symbol) %>% 
  summarise(r_min=min(monthly_returns), r_max=max(monthly_returns), r_median=median(monthly_returns), r_mean=mean(monthly_returns), r_sd=sd(monthly_returns))

# print table
summarised_returns %>%
  kbl(col.names = c("Ticker", "Min return", "Max return", "Median return", "Avg. return", "SD return")) %>%
  kable_classic(c("hover"), full_width = F, html_font = "Cambria") %>%
  kable_styling()

```


## Density plot  

### Plot

**- Plot a density plot, using `geom_density()`, for each of the stocks**

```{r density_monthly_returns, fig.width=10,fig.height=5}

# create table with SD returns in descending order to observe riskiest stock
summarised_returns %>% 
  arrange(desc(r_sd)) %>% 
  kbl(col.names = c("Ticker", "Min return", "Max return", "Median return", "Avg. return", "SD return")) %>%
  kable_classic(c("hover"), full_width = F, html_font = "Cambria") %>%
  kable_styling()

# create ggplot based on monthly returns
ggplot(myStocks_returns_monthly, aes(x=monthly_returns, color=symbol)) +
         geom_density() +
  theme(axis.title.x = element_text()) +
  labs(title = "Standard deviation of monthly returns against average return\n",
         x = "Monthly returns\n", 
         y = "\nAverage return")

```


### Interpretation  

**- What can you infer from this plot? Which stock is the riskiest? The least risky?** 

*ANSWER:*    
     
Generally, the monthly returns are normally distributed around 0. Stocks with a low standard deviation of monthly returns show a high peak in the plot and are therefore considered less risky. On the other hand, stocks which have strongly deviating returns from month to month show a more spread out bell shape and thus are considered more risky. More specifically, DOW and AAPL are among the riskiest stocks while JNJ and SPY bear the least risk.

## Relationship between expected monthly return and risk

### Plot  

**- Finally, produce a plot that shows the expected monthly return (mean) of a stock on the Y axis and the risk (standard deviation) in the X-axis. Please use `ggrepel::geom_text_repel()` to label each stock with its ticker symbol**

```{r risk_return_plot, fig.width=10,fig.height=5}

# create ggplot with trend line indicating the expected relationship between risk and return
ggplot(summarised_returns, aes(label=symbol, x=r_sd, y=r_mean)) +
  geom_point() +
  ggrepel::geom_label_repel(mapping = NULL, data = NULL, stat = "identity",
  position = "identity") +
  geom_smooth(method='lm', formula= y~x) +
  theme(axis.title.x = element_text()) +
  labs(title = "Relationship between average return and risk (SD)\n",
         x = "Risk (SD)\n", 
         y = "\nAverage return")

```

### Interpretation

**- What can you infer from this plot? Are there any stocks which, while being riskier, do not have a higher expected return?**

*ANSWER:*    
   
Yes. DOW is the riskiest stock but it has a comparably low return. This does not fit the general relationship between risk and return.

# On your own: IBM HR Analytics

```{r}

# import data & initial data exploration
hr_dataset <- read_csv(file="data/datasets_1067_1925_WA_Fn-UseC_-HR-Employee-Attrition.csv")
glimpse(hr_dataset)

```

```{r}

# cleaning of data set as variable names are in capital letters, some variables are not really necessary, and some variables, e.g., `education` are given as a number rather than a more useful description

hr_cleaned <- hr_dataset %>% 
  clean_names() %>% 
  mutate(
    education = case_when(
      education == 1 ~ "Below College",
      education == 2 ~ "College",
      education == 3 ~ "Bachelor",
      education == 4 ~ "Master",
      education == 5 ~ "Doctor"
    ),
    environment_satisfaction = case_when(
      environment_satisfaction == 1 ~ "Low",
      environment_satisfaction == 2 ~ "Medium",
      environment_satisfaction == 3 ~ "High",
      environment_satisfaction == 4 ~ "Very High"
    ),
    job_satisfaction = case_when(
      job_satisfaction == 1 ~ "Low",
      job_satisfaction == 2 ~ "Medium",
      job_satisfaction == 3 ~ "High",
      job_satisfaction == 4 ~ "Very High"
    ),
    performance_rating = case_when(
      performance_rating == 1 ~ "Low",
      performance_rating == 2 ~ "Good",
      performance_rating == 3 ~ "Excellent",
      performance_rating == 4 ~ "Outstanding"
    ),
    work_life_balance = case_when(
      work_life_balance == 1 ~ "Bad",
      work_life_balance == 2 ~ "Good",
      work_life_balance == 3 ~ "Better",
      work_life_balance == 4 ~ "Best"
    )
  ) %>% 
  select(age, attrition, daily_rate, department,
         distance_from_home, education,
         gender, job_role,environment_satisfaction,
         job_satisfaction, marital_status,
         monthly_income, num_companies_worked, percent_salary_hike,
         performance_rating, total_working_years,
         work_life_balance, years_at_company,
         years_since_last_promotion)

```

## Summary 
  
Produce a one-page summary describing this dataset.

1. How often do people leave the company (`attrition`)

```{r}

# table transformation
attrition_rate <- hr_cleaned %>% 
  count(hr_cleaned$attrition) %>% 
  mutate(n/sum(n)*100)

# print table
attrition_rate %>%
  kbl(col.names = c("Attrition", "Count", "Percentage of total")) %>%
  kable_classic(c("hover"), full_width = F, html_font = "Cambria") %>%
  kable_styling()
```

2. How are `age`, `years_at_company`, `monthly_income` and `years_since_last_promotion` distributed? can you roughly guess which of these variables is closer to Normal just by looking at summary statistics? 

*ANSWER:*
With just looking at the summary statistics (skim) it seems that age is the only one that is roughly normally distributed. The subsequent plots confirm this hypotheses. 

```{r, fig.width=10,fig.height=5}
# looking at the summary statistics
skim(hr_cleaned)

# create ggplot for Age distribution
ggplot(hr_cleaned, aes(age)) +
  labs(title = "\nAge Distribution Density plot",  x = "\nAge", y= "Density\n") +
  geom_density() +
  theme_economist() 

# create ggplot for company tenure distribution
ggplot(hr_cleaned,aes(x=years_at_company)) +
  labs(title = "Company tenure Density plot\n",  x = "\nYears", y= "Density\n") +
  geom_density() +
  theme_economist()

# create ggplot for monthly income distribution
ggplot(hr_cleaned,aes(x=monthly_income)) + 
  labs(title = "Monthly income Density plot\n",  x = "\nIncome", y= "Density\n") +
  geom_density() +
  theme_economist()

# create ggplot for last promotion distribution
ggplot(hr_cleaned,aes(x=years_since_last_promotion)) +
  labs(title = "Years since last promotion Density plot\n",  x = "\nYears since last promotion", y="Density\n") +
  geom_density() +
  theme_economist()
```

3. How are `job_satisfaction` and `work_life_balance` distributed? Don't just report counts, but express categories as % of total

```{r wlb, fig.width=10,fig.height=5}

# define 1. rating order
Rating_order1 <- c('Low', 'Medium', 'High', 'Very High')

# create ggplot in % of total of job satisfaction
ggplot(hr_cleaned, aes(x = factor(job_satisfaction, level = Rating_order1))) +
  geom_bar(aes(y=(..count..)/sum(..count..))) +
  labs(title = "Employee job satisfaction distribution\n", x = "\nJob satisfaction", y = "Percentage\n") +
   scale_y_continuous(labels = scales::percent) +
  theme_economist()

# define 1. rating order
Rating_order2 <- c('Bad', 'Good', 'Better', 'Best')

# create ggplot in % of total of work-life balance 
ggplot(hr_cleaned, aes(x = factor(work_life_balance, level = Rating_order2))) +
   geom_bar(aes(y=(..count..)/sum(..count..))) +
  labs(title = "Employee work-life-balance Distribution\n", x = "\nWork-life-balance", y = "Percentage\n") +
   scale_y_continuous(labels = scales::percent) +
  theme_economist()

```

4. Is there any relationship between monthly income and education? Monthly income and gender?

```{r, fig.width=10,fig.height=5}

# define level order
level_order <- c('Below College', 'College', 'Bachelor', 'Master', 'Doctor')

# create ggplot for relationship of education and income
ggplot(hr_cleaned, aes(x = factor(education, level = level_order), y = monthly_income)) + 
  geom_boxplot() +
  labs(title = "Education and income relationship\n", x = "\nEducation", y = "Monthly income (USD)\n") +
 theme_economist()

# create ggplot for relationship of gender and income
ggplot(hr_cleaned, aes(x = gender, y = monthly_income)) + 
  geom_boxplot() +
  labs(title = "Relationship between gender and income\n", x = "\nGender", y = "Monthly income (USD)\n") +
  theme_economist()

```

5. Plot a boxplot of income vs job role. Make sure the highest-paid job roles appear first

```{r, fig.width=10,fig.height=5}

# create ggplot for monthly income distribution by job title
ggplot(hr_cleaned, aes(x = reorder(job_role, -monthly_income), y = monthly_income)) + 
  geom_boxplot() +
  labs(title = "Monthly income distribution by job title\n", x = "\nJob title", y = "Monthly income ($)\n") +
   theme_economist() +
  theme(legend.position = "none",
           axis.title.x = element_text(), axis.text.x = element_text(angle = 90)) 

```

6. Calculate and plot a bar chart of the mean (or median?) income by education level.

```{r, fig.width=10,fig.height=5}

# create ggplot for median income
Income_by_education <- hr_cleaned %>%
  group_by(education) %>%
  summarize(median = median(monthly_income))
  ggplot(Income_by_education, aes(x = reorder(education, median), y = median)) +
  geom_bar(stat = 'identity', show.legend = F) +
  labs(title = "Median income by educational level\n", x = "\nEducational level", y = "Monthly income (USD)\n") +
  theme_economist()
  
# create ggplot for mean income
Income_by_education <- hr_cleaned %>%
  group_by(education) %>%
  summarize(mean = mean(monthly_income))
  ggplot(Income_by_education, aes(x = reorder(education, mean), y = mean)) +
  geom_bar(stat = 'identity', show.legend = F) +
  labs(title = "Mean income by educational level\n", x = "\nEducational level", y = "Monthly income (USD)\n") +
  theme_economist()

```
 
7. Plot the distribution of income by education level. Use a facet_wrap and a theme from `ggthemes`

```{r, fig.width=10,fig.height=5}

# create ggplot 
ggplot(hr_cleaned, aes(x = monthly_income)) +
  geom_density() +
  facet_wrap(~education) +
  labs(title = "Income distribution by educational level\n", x = "\nIncome (USD)", y = "Density\n") +
  theme_economist()
  
```

8. Plot income vs age, faceted by `job_role`

```{r, fig.width=10,fig.height=5}

# create ggplot faceted by job role
ggplot(hr_cleaned, aes(x = age, y = monthly_income)) +
  geom_point(show.legend = F) +
  geom_smooth() +
  facet_wrap(~job_role) +
  labs(title = "") +
  labs(title = "Monthly Income by job role and age\n", x = "\nAge", y = "Monthly income\n") +
  theme_economist()

```

# Challenge 1: Replicating a chart

The purpose of this exercise is to make a publication-ready plot using your `dplyr` and `ggplot2` skills. Open the journal article "Riddell_Annals_Hom-Sui-Disparities.pdf". Read the abstract and have a look at Figure 3. The data you need is "CDC_Males.csv".

```{r challenge1, out.width="90%"}
knitr::include_graphics(here::here("images", "figure3.jpeg"), error = FALSE)

# import data
firearm <- read_csv(file="data/CDC_Males.csv")

# table transformation to drop NAs and only consider firearm-related data points
adj_firearm <- firearm %>% 
  filter(!is.na(gun.house.prev.category)) %>% 
  filter(type.fac == "Firearm-related")

```


```{r, fig.width=10,fig.height=5}

# create ggplot to replicate figure 3
ggplot(adj_firearm, aes(x = adjusted.suicide.White, y = adjusted.homicide.White, label = ST)) +
  geom_point(aes(fill = factor(gun.house.prev.category), # points in color based on category
                 size = average.pop.white), pch = 21) + # size depending on population size
  scale_fill_manual(values = c("#fdf0d9", "#fdcc8a", "#fb8d58", "#d6301e"), name = "Gun ownership") + # set exact color code
  scale_size_continuous(name = "White population", breaks = c(500000, 1500000, 3000000, 7000000), # define legend breaks
                      labels = c("   500k", "1.5m", "3m", "7m"), range = c(1,15), # manually insert format
                      limits = c(0, 8000000)) +
  labs(x = "White Suicide Rate (per 100,000 per year)", y = " White Homicide Rate (per 100,000 per year)") +
  ggrepel::geom_text_repel() +
  annotate(geom="text", x=25, y=0.75, size =3, label="Spearman' correlation's rho: 0.74") + # add correlation text box
  theme(panel.grid.major = element_line(colour = "#f0f0f0"), # define grid
        panel.background = element_rect(colour = "black", size=0.5, fill = NA), # set borders
        legend.key = element_rect(colour = "transparent", fill = "transparent")) +
  guides(fill = guide_legend(order = 1, override.aes = list(size = 5)), # define order and size of legend
         size = guide_legend(order = 2)) +
  labs(title = "Homicide and Suicide Rates in Black Versus White Non-Hispanic Men",
       subtitle = "Relationship between the annual rates of firearm homicide and suicide among white men, by state, and reported \nhousehold firearm ownership, 2008 to 2016.")
```


# Challenge 2: 2016 California Contributors plots

As discussed in class, I would like you to reproduce the plot that shows the top ten cities in highest amounts raised in political contributions in California during the 2016 US Presidential election.

```{r challenge2, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "challenge2.png"), error = FALSE)
```

```{r, load_CA_data, warnings= FALSE, message=FALSE}
# Make sure you use vroom() as it is significantly faster than read.csv()
CA_contributors_2016 <-  vroom::vroom(file="data/CA_contributors_2016.csv")
zip <-  vroom::vroom(file="data/zip_code_database.csv")

```



```{r, warnings= FALSE, message=FALSE}

# table 1 transformation to group by zip and candidate
adj_CA_contributions_2016 <- CA_contributors_2016 %>% 
  group_by(zip, cand_nm) %>% 
  summarise(total_cont = sum(contb_receipt_amt))

# table 2 transformation to filter only for the zips in California
adj_zip <- zip %>% 
  filter(state == "CA") %>%
  transform(zip = as.double(zip)) # transform zip data points to double (instead of character)

#join the two tables
joined_table <- adj_CA_contributions_2016 %>%
  inner_join(adj_zip, by ="zip") %>% 
  group_by(primary_city, cand_nm) %>%
  summarise(total_cont = sum(total_cont)) 
  

# table to show total_cont on a city level
joined_table_city <- joined_table %>% 
  group_by(primary_city) %>% 
  summarise(total_cont = sum(total_cont)) %>% 
  mutate(total_cont = total_cont/1000000)%>%
  arrange(desc(total_cont)) %>%
  top_n(10)

# create ggplot on city level
ggplot(joined_table_city, aes(x=reorder(primary_city, total_cont), y=total_cont)) +
  geom_bar(stat="identity")+
  coord_flip() +
  theme_economist() +
  scale_y_continuous(labels = scales::dollar) +
  theme(legend.position = "none",
           axis.title.x = element_text()) + 
  labs(title = "Top 10 cities by total contributions in California\n",
         x = "City\n", 
         y = "\nTotal contributions in millions")
```


```{r, out.width="100%"}

# import required libraries
library(patchwork)
library(tidytext)

# table transformation to filter out only trump and clinton
comparison_table <- joined_table %>%
  filter(cand_nm == "Trump, Donald J." | cand_nm == "Clinton, Hillary Rodham") %>%
  group_by(cand_nm) %>%
  top_n(10, total_cont)

# create ggplot to imitate graph from lecture
ggplot(comparison_table, aes(x=reorder_within(primary_city, total_cont, cand_nm), y = total_cont)) +
  geom_bar(stat='identity', aes(color = as.factor(cand_nm), fill = as.factor(cand_nm)), show.legend = FALSE) +
  coord_flip() + # flip axis
  labs(title = "Where did candidates raise most money\n", y = "Amount raised") +
  scale_fill_manual(values = c("#2c7fb8", "#cb181d")) +
  scale_color_manual(values = c("#2171b5", "#d7301f")) +
  scale_y_continuous(labels = scales::dollar) + # turn y axis to USD
  scale_x_reordered() +
  facet_wrap(~cand_nm, scales = "free") +
  theme(strip.background = element_rect(fill="lightGrey", color = "black", size = 0.3), # set box with candidate names
        plot.title = element_text(size = 5), 
              axis.title.y = element_blank()) + theme(plot.title = element_text(size = 10),
        panel.background = element_rect(fill = "white"), # define coordinate fields
        panel.grid.major = element_line(colour = "#f0f0f0"), # set grid
        strip.text = element_text(size=6), # text of names
              panel.border = element_rect(colour = "black", size=0.3, fill = NA, linetype = "solid") # border line
              )
```

# Details

- Who did you collaborate with: Muhammad Nauman Alam Khan, Tom Invernizzi, Rayna Zhang, Jerome Billiet, Christopher Baumann
- Approximately how much time did you spend on this problem set: Approx. 12h
- What, if anything, gave you the most trouble: the challenges

